[
  {
    "objectID": "nlp_linkedin_examples.html",
    "href": "nlp_linkedin_examples.html",
    "title": "NLP Question Answering and Zero-Shot Classification",
    "section": "",
    "text": "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\nfrom transformers import pipeline\nfrom fastai.tabular.all import *\nimport plotly.express as px"
  },
  {
    "objectID": "nlp_linkedin_examples.html#create-a-classifier-to-run-zero-shot-classification",
    "href": "nlp_linkedin_examples.html#create-a-classifier-to-run-zero-shot-classification",
    "title": "NLP Question Answering and Zero-Shot Classification",
    "section": "Create a classifier to run â€œzero-shotâ€ classification",
    "text": "Create a classifier to run â€œzero-shotâ€ classification\n> Here we'll use the hugging face transformers `pipeline` to use the pretrained zero-shot-classification \"auto-model.\"  This instantiates a class and automatically uses the bart-large-mnli model to classify how relevent different topics are to any given text\n\nclassifier = pipeline(\"zero-shot-classification\")\n\nNo model was supplied, defaulted to facebook/bart-large-mnli (https://huggingface.co/facebook/bart-large-mnli)\n\n\n\nTopic Relevance Example 1\n\nHow relevant are the following topics: Politics, Public Health, Economics\n\n\nsequence = \"Who are you voting for in 2022?\"\ncandidate_labels = [\"politics\", \"public health\", \"economics\"]\n\nprint(classifier(sequence, candidate_labels))\n\n{'sequence': 'Who are you voting for in 2022?', 'labels': ['politics', 'economics', 'public health'], 'scores': [0.9570426940917969, 0.023699436336755753, 0.019257841631770134]}\n\n\n\n\nTopic Relevance Example 2\n\nUse this on data from a Linkedin dataset to see if linkedin people are relevant\n\nBring in a Linkedin dataset\n\npath = Path('..')\ndf = pd.read_csv(path/'linkedin.csv')\n\nCreate a column with all info to easily feed into the models\n\ndf.fillna('',inplace=True) #fill all na with ''\n#create input col to feed into the models\ndf['input'] = 'description: ' + df.description+'; Experience: '+df.Experience+ '; Name: '+ df.Name+'; position: '+df.position+'; location: '+df.location+ '; skills: '+df.skills+ '; clean_skills: '+df.clean_skills\ndf.input = df.input.astype(str)\ndf.input = df.input.str.replace('\\\\n',' ',regex=False);  print(str(df.input[0][:400])+'...')\n\ndescription: An experienced HR professional,  HR mentor and Coach , Talent advisory and HR strategist... see more; Experience: Senior Vice President & Head of HRCompany NameSamsung Electronics India LimitedDates EmployedJan 2018 â€“ PresentEmployment Duration2 yrs 3 mosLocationGurgaon, Haryana, IndiaVice President Franchise capability building and business transformationCompany NameCoca-Cola India a...\n\n\nCreate a list of topics/categories. Weâ€™ll use th emodel to check how relevant each topic/category is to each person. In this case each person is text entry from the column input.\n\ncandidate_labels = df.category.unique().tolist()\ncandidate_labels.extend(['US','India']); 'first five categories: '+str(candidate_labels[:5])\n\n\"first five categories: ['HR', 'Designing', 'Managment', 'Information Technology', 'Education']\"\n\n\n\nsource\n\n\nanalyze_one\n\n analyze_one (df:pandas.core.frame.DataFrame, candidate_labels, index)\n\n\n\n\n\nType\nDetails\n\n\n\n\ndf\nDataFrame\ndataframe with df.input\n\n\ncandidate_labels\n\n\n\n\nindex\n\n\n\n\n\n\n# analyze_one(df,candidate_labels,index=0)\n\n\n# analyze_one(df,candidate_labels,index=1000)\n\n\n# analyze_one(df,candidate_labels,index=421)\n\n\n# df.groupby('category').Name.count()\n\n\n# df.loc[df.category=='Agricultural'].description\n\nTry some questions about a text\n\nidx = 4\ntext = df.input[idx][:(int(.75 *len(df.input[idx])))]; print(text)\n\ndescription: Over 18 Years of experience in IT /ITES  / BPO with leading global OrganizationsHave a passion for working on great products, enthusiastic about #UserExperience #SaaS #HRTech #Bots #Io...\n            see more; Experience: Company NameEXLTotal Duration6 yrs 4 mosTitleVice President - Head of Digital HR Technologies and HR Operations/ shared servicesDates EmployedJul 2018 â€“ PresentEmployment Duration1 yr 9 mosLocationNoida Area, IndiaHave a passion for working on great products, enthusiastic about #UserExperience #SaaS #HRTech #Bots #IoT #Gadgets, #Mobileapps, #ERP... Strong experience in managing Transformative Business HR IT initiatives in a Global Shared Service environmentTitleSenior Assistant Vice President - Human ResourcesDates EmployedDec 2013 â€“ Jun 2018Employment Duration4 yrs 7 mosTitleVice President - Head of Digital HR Technologies and HR Operations/ shared servicesDates EmployedJul 2018 â€“ PresentEmployment Duration1 yr 9 mosLocationNoida Area, IndiaHave a passion for working on great products, enthusiastic about #UserExperience #SaaS #HRTech #Bots #IoT #Gadgets, #Mobileapps, #ERP... Strong experience in managing Transformative Business HR IT initiatives in a Global Shared Service environmentT; Name: Rakesh Kumar; position: Vice President - Digital HR Transformation Lead, Global HR Operations / Shared Services and HR Technologies; location: Central Delhi, Delhi, India; skills: [' Team Management ', ' Human Resources ', ' Employee Engagement ', ' Talent Acquisition ', ' Deferred Compensation ', ' ERP ', ' SDLC ', ' HR Consulting ', ' Change Management ', ' Strategic HR ', ' Business Process Improvement ', ' MIS ', ' HRIS ', ' PeopleSoft ', ' Cognos ', ' Ma\n\n\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n\ntext = text\n\nquestions = [\"What is the persons name?\" ,\"What companies has this person worked with?\",\"Do they work in HR?\"\n]\n\nfor question in questions:\n    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"pt\")\n    input_ids = inputs[\"input_ids\"].tolist()[0]\n\n    outputs = model(**inputs)\n    answer_start_scores = outputs.start_logits\n    answer_end_scores = outputs.end_logits\n\n    # Get the most likely beginning of answer with the argmax of the score\n    answer_start = torch.argmax(answer_start_scores)\n    # Get the most likely end of answer with the argmax of the score\n    answer_end = torch.argmax(answer_end_scores) + 1\n\n    answer = tokenizer.convert_tokens_to_string(\n        tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n    )\n\n    print(f\"Question: {question}\")\n    print(f\"Answer: {answer}\")\n    print()\n\nQuestion: What is the persons name?\nAnswer: rakesh kumar\n\nQuestion: What companies has this person worked with?\nAnswer: leading global organizations\n\nQuestion: Do they work in HR?\nAnswer: [CLS]\n\n\n\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n\ntext = r\"\"\"\nðŸ¤— Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\narchitectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural\nLanguage Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\nTensorFlow 2.0 and PyTorch.\n\"\"\"\n\nquestions = [\n    \"How many pretrained models are available in ðŸ¤— Transformers?\",\n    \"What does ðŸ¤— Transformers provide?\",\n    \"ðŸ¤— Transformers provides interoperability between which frameworks?\",\n]\n\nfor question in questions:\n    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"pt\")\n    input_ids = inputs[\"input_ids\"].tolist()[0]\n\n    outputs = model(**inputs)\n    answer_start_scores = outputs.start_logits\n    answer_end_scores = outputs.end_logits\n\n    # Get the most likely beginning of answer with the argmax of the score\n    answer_start = torch.argmax(answer_start_scores)\n    # Get the most likely end of answer with the argmax of the score\n    answer_end = torch.argmax(answer_end_scores) + 1\n\n    answer = tokenizer.convert_tokens_to_string(\n        tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n    )\n\n    print(f\"Question: {question}\")\n    print(f\"Answer: {answer}\")\n    print()\n\nQuestion: How many pretrained models are available in ðŸ¤— Transformers?\nAnswer: over 32 +\n\nQuestion: What does ðŸ¤— Transformers provide?\nAnswer: general - purpose architectures\n\nQuestion: ðŸ¤— Transformers provides interoperability between which frameworks?\nAnswer: tensorflow 2. 0 and pytorch"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nfoo\n\n foo ()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "zero-shot-nlp-3",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "zero-shot-nlp-3",
    "section": "Install",
    "text": "Install\npip install zero_shot_nlp_3"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "zero-shot-nlp-3",
    "section": "How to use",
    "text": "How to use\nUse this to either Ask Questions about a piece of text, or to Check the Categories in a Text\n\n1+1\n\n2"
  }
]